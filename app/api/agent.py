import os
import json
from dotenv import load_dotenv
from openai import AsyncOpenAI
from mcp import ClientSession

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"


if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY is not set in .env file")

# LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (Google Gemini)
client = AsyncOpenAI(
    api_key=GEMINI_API_KEY,
    base_url=GEMINI_BASE_URL
)

# âœ… ì „ì—­ MCP ì„¸ì…˜ ë³€ìˆ˜
mcp_session: ClientSession | None = None


async def run_ai_agent(user_query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ Gemini + MCP ë„êµ¬ë¥¼ í™œìš©í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global mcp_session

    if not mcp_session:
        return "âš ï¸ Error: MCP Server is not connected. Please check backend logs."

    try:
        # 1. ë„êµ¬ ëª©ë¡ ì¡°íšŒ
        print("ğŸ” [Agent] Fetching tools list...")
        tools_list = await mcp_session.list_tools()

        openai_tools = []
        for tool in tools_list.tools:
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            })

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        messages = [
            {
                "role": "system",
                "content": "You are a helpful AI assistant. You have access to tools, but you should only use them when necessary. If the user asks a general question (like 'Hi' or 'What is Python?'), answer directly without using tools."
            },
            {"role": "user", "content": user_query}
        ]

        print(f"ğŸš€ [Agent] Sending query to Gemini: {user_query}")

        # 2. Gemini 1ì°¨ ì¶”ë¡  (Reasoning)
        response = await client.chat.completions.create(
            model="gemini-2.5-flash-lite",
            messages=messages,
            tools=openai_tools,
            tool_choice="auto"
        )

        message = response.choices[0].message
        print(f"ğŸ§ [Agent] First Response: Content={message.content}, Tool_Calls={message.tool_calls}")

        # 3. ë„êµ¬ í˜¸ì¶œ í•„ìš” ì—¬ë¶€ í™•ì¸
        if message.tool_calls:
            print("ğŸ› ï¸ [Agent] Tool usage detected!")
            for tool_call in message.tool_calls:
                func_name = tool_call.function.name
                func_args = json.loads(tool_call.function.arguments)

                # MCP ì„œë²„ë¡œ ë„êµ¬ ì‹¤í–‰ ìš”ì²­
                print(f"   -> Calling tool: {func_name} with {func_args}")
                result = await mcp_session.call_tool(func_name, arguments=func_args)

                # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                messages.append(message)
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": str(result.content)
                })

            # 4. ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
            final_response = await client.chat.completions.create(
                model="gemini-2.5-flash-lite",
                messages=messages
            )
            return final_response.choices[0].message.content or "Error: Empty response after tool use."

        # ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ ëŒ€í™”)
        if message.content:
            return message.content

        return "ğŸ¤” AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (Content is None)"

    except Exception as e:
        print(f"âŒ [Agent Error] {e}")
        return f"An error occurred while processing your request: {e}"