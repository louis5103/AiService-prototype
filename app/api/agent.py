import os
import json
from dotenv import load_dotenv
from openai import AsyncOpenAI
from mcp import ClientSession
from mcp.client.sse import sse_client
from fastapi import HTTPException

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
MCP_SERVER_URL = "http://localhost:8080/sse"

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY is not set in .env file")

# LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (Google Gemini)
client = AsyncOpenAI(
    api_key=GEMINI_API_KEY,
    base_url=GEMINI_BASE_URL
)

# ì „ì—­ MCP ì„¸ì…˜ ë³€ìˆ˜
mcp_session: ClientSession | None = None
_sse_context = None  # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ìœ ì§€ë¥¼ ìœ„í•œ ë³€ìˆ˜


async def initialize_mcp_connection():
    """MCP ì„œë²„ì™€ ì—°ê²°ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤ (Lifespanì—ì„œ í˜¸ì¶œ)"""
    global mcp_session, _sse_context

    try:
        # SSE ì—°ê²° ì‹œì‘
        _sse_context = sse_client(MCP_SERVER_URL)
        read, write = await _sse_context.__aenter__()

        # ì„¸ì…˜ ì´ˆê¸°í™”
        session = ClientSession(read, write)
        await session.__aenter__()
        await session.initialize()

        mcp_session = session
        print(f"âœ… Connected to MCP Server at {MCP_SERVER_URL}")

    except Exception as e:
        print(f"âŒ Failed to connect to MCP Server: {e}")
        # ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” ì—¬ê¸°ì„œ ì¬ì‹œë„ ë¡œì§ ë“±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ


async def shutdown_mcp_connection():
    """MCP ì„œë²„ì™€ ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤"""
    global mcp_session, _sse_context

    if mcp_session:
        await mcp_session.__aexit__(None, None, None)
    if _sse_context:
        await _sse_context.__aexit__(None, None, None)
    print("ğŸ›‘ MCP Server Disconnected")


async def run_ai_agent(user_query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ Gemini + MCP ë„êµ¬ë¥¼ í™œìš©í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global mcp_session

    if not mcp_session:
        return "âš ï¸ Error: MCP Server is not connected. Please check backend logs."

    # 1. ë„êµ¬ ëª©ë¡ ì¡°íšŒ
    tools_list = await mcp_session.list_tools()

    openai_tools = []
    for tool in tools_list.tools:
        openai_tools.append({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema
            }
        })

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    # ëª¨ë¸ì—ê²Œ "ë„êµ¬ê°€ í•„ìš” ì—†ìœ¼ë©´ ê·¸ëƒ¥ ëŒ€í™”í•´"ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ì§€ì‹œí•©ë‹ˆë‹¤.
    messages = [
        {
            "role": "system",
            "content": "You are a helpful AI assistant. You have access to tools, but you should only use them when necessary. If the user asks a general question (like 'Hi' or 'What is Python?'), answer directly without using tools."
        },
        {"role": "user", "content": user_query}
    ]

    print(f"ğŸš€ [Agent] Sending query to Gemini: {user_query}")  # ë¡œê·¸ ì¶”ê°€

    # 2. Gemini 1ì°¨ ì¶”ë¡  (Reasoning)
    response = await client.chat.completions.create(
        model="gemini-2.5-flash-lite",
        messages=messages,
        tools=openai_tools,
        tool_choice="auto"
    )

    message = response.choices[0].message
    print(f"ğŸ§ [Agent] First Response: Content={message.content}, Tool_Calls={message.tool_calls}")  # ë””ë²„ê¹… ë¡œê·¸

    # 3. ë„êµ¬ í˜¸ì¶œ í•„ìš” ì—¬ë¶€ í™•ì¸
    if message.tool_calls:
        print("ğŸ› ï¸ [Agent] Tool usage detected!")
        for tool_call in message.tool_calls:
            func_name = tool_call.function.name
            func_args = json.loads(tool_call.function.arguments)

            # MCP ì„œë²„ë¡œ ë„êµ¬ ì‹¤í–‰ ìš”ì²­
            result = await mcp_session.call_tool(func_name, arguments=func_args)

            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            messages.append(message)
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": str(result.content)
            })

        # 4. ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
        final_response = await client.chat.completions.create(
            model="gemini-2.5-flash-lite",
            messages=messages
        )
        return final_response.choices[0].message.content or "Error: Empty response after tool use."

    # ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ ëŒ€í™”)
    # message.contentê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    if message.content:
        return message.content

    # ë§Œì•½ ë„êµ¬ë„ ì•ˆ ë¶€ë¥´ê³  ë‚´ìš©ë„ ì—†ì„ ë•Œ.
    return "ğŸ¤” AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (Content is None)"