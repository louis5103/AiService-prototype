import os
import json
from pathlib import Path
from openai import OpenAI
from mcp import ClientSession
from dotenv import load_dotenv
from app.api.schemas import ChatMessage

BASE_DIR = Path(__file__).resolve().parent.parent.parent
load_dotenv(dotenv_path=BASE_DIR / ".env")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

client = OpenAI(
    api_key=GEMINI_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì•Œë¼ë”˜ ì„œì ì˜ ì „ë¬¸ AI ì‚¬ì„œì…ë‹ˆë‹¤.

[í•µì‹¬ ì—­í• ]
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì œê³µëœ 'System Context(í•„í„° ì •ë³´)'ë¥¼ ê²°í•©í•˜ì—¬ ìµœì ì˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
2. 'search_books' ë„êµ¬ì˜ 'filters' ì¸ìë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.

[ë°ì´í„° í•´ì„ ê°€ì´ë“œ]
- ë„êµ¬ ê²°ê³¼ì— 'âœ…[ì‹¤ì‹œê°„]' ë§ˆí¬ê°€ ìˆë‹¤ë©´, ì´ëŠ” 100% ì •í™•í•œ í˜„ì¬ ì •ë³´ì…ë‹ˆë‹¤.
- 'íŒë§¤ì§€ìˆ˜'ê°€ 50,000 ì´ìƒì´ë©´ [ì´ˆëŒ€ë°• ë² ìŠ¤íŠ¸ì…€ëŸ¬], 10,000 ì´ìƒì´ë©´ [ìŠ¤í…Œë””ì…€ëŸ¬]ë¡œ ì†Œê°œí•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ 'ìµœì‹  íŠ¸ë Œë“œ'ë¥¼ ë¬¼ìœ¼ë©´ ì¶œê°„ì¼ê³¼ íŒë§¤ì§€ìˆ˜ë¥¼ ê·¼ê±°ë¡œ ì¶”ì²œí•˜ì„¸ìš”.

[ë‹µë³€ ìŠ¤íƒ€ì¼]
- ì±… ì œëª©, ì €ì, ê°€ê²©ì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ê³ , ì¶”ì²œ ì´ìœ ë¥¼ ë§ë¶™ì´ì„¸ìš”.
"""


async def run_ai_agent(user_query: str, chat_history: list[ChatMessage], session: ClientSession) -> str:
    try:
        mcp_tools = await session.list_tools()
        openai_tools = [{"type": "function",
                         "function": {"name": t.name, "description": t.description, "parameters": t.inputSchema}} for t
                        in mcp_tools.tools]

        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + \
                   [{"role": m.role, "content": m.content} for m in chat_history] + \
                   [{"role": "user", "content": user_query}]

        response = client.chat.completions.create(
            model="gemini-2.0-flash-exp", messages=messages, tools=openai_tools
        )
        assistant_msg = response.choices[0].message

        if assistant_msg.tool_calls:
            messages.append(assistant_msg)
            for tool_call in assistant_msg.tool_calls:
                t_name = tool_call.function.name
                t_args = json.loads(tool_call.function.arguments)
                print(f"ğŸ¤– Tool Call: {t_name} | Args: {t_args}")

                result = await session.call_tool(t_name, arguments=t_args)
                messages.append({"role": "tool", "tool_call_id": tool_call.id, "content": result.content[0].text})

            final_res = client.chat.completions.create(
                model="gemini-2.0-flash-exp", messages=messages
            )
            return final_res.choices[0].message.content

        return assistant_msg.content
    except Exception as e:
        print(f"âŒ Error: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤, ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."