import os
import json
from openai import OpenAI
from mcp import ClientSession

# ìŠ¤í‚¤ë§ˆ ì„í¬íŠ¸
from app.api.schemas import ChatMessage
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
client = OpenAI(
    api_key=GEMINI_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì•Œë¼ë”˜ ì„œì ì˜ ìœ ëŠ¥í•œ AI ì‚¬ì„œì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ [ë§¥ë½ ì¶”ì²œ]ê³¼ [í‚¤ì›Œë“œ ê²€ìƒ‰]ì„ êµ¬ë¶„í•˜ê³ , 
ëŒ€í™” ì†ì— ìˆ¨ê²¨ì§„ [í•„í„° ì¡°ê±´]ì„ ì¶”ì¶œí•˜ì—¬ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.

[ë„êµ¬ ì‚¬ìš© ì „ëµ]
1. 'search_books' ë„êµ¬ í˜¸ì¶œ ì‹œ:
   - search_type="context": "ì¬ë°ŒëŠ” ì†Œì„¤ ì¶”ì²œí•´ì¤˜", "ìë°” ê³µë¶€í•˜ê³  ì‹¶ì€ë°" (ì˜ë¯¸/ì¶”ì²œ)
   - search_type="keyword": "í•œê°• ì‘ê°€ ì±… ì°¾ì•„ì¤˜", "í† ë¹„ì˜ ìŠ¤í”„ë§" (ì •í™•í•œ ê²€ìƒ‰)
   - filters: ì‚¬ìš©ìê°€ "3ë§Œì› ì´í•˜", "IT ë¶„ì•¼" ë“±ì„ ì–¸ê¸‰í•˜ë©´ {'max_price': 30000} ì²˜ëŸ¼ í¬í•¨.

2. 'get_details' ë„êµ¬:
   - íŠ¹ì • ì±…ì˜ ìƒì„¸ ì •ë³´(ì¬ê³ , ë¦¬ë·° ë“±)ê°€ í•„ìš”í•  ë•Œ ISBNìœ¼ë¡œ í˜¸ì¶œ.

[ë‹µë³€ ìŠ¤íƒ€ì¼]
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì‚¬ì„œì²˜ëŸ¼ ë‹µë³€í•˜ì„¸ìš”.
"""


async def run_ai_agent(user_query: str, chat_history: list[ChatMessage], session: ClientSession) -> str:
    """
    main.pyì—ì„œ ì—°ê²°ëœ sessionì„ ì¸ìë¡œ ë°›ì•„ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        mcp_tools = await session.list_tools()
        openai_tools = [{
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema
            }
        } for tool in mcp_tools.tools]

        # 2. ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        for msg in chat_history:
            messages.append({"role": msg.role, "content": msg.content})

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": user_query})

        # 3. 1ì°¨ LLM í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gemini-2.0-flash-exp",
            messages=messages,
            tools=openai_tools,
        )

        assistant_msg = response.choices[0].message

        # 4. ë„êµ¬ í˜¸ì¶œ í™•ì¸
        if assistant_msg.tool_calls:
            messages.append(assistant_msg)  # ëŒ€í™” ë§¥ë½ ìœ ì§€

            for tool_call in assistant_msg.tool_calls:
                tool_name = tool_call.function.name
                try:
                    tool_args = json.loads(tool_call.function.arguments)
                except:
                    tool_args = {}

                print(f"ğŸ¤– [Agent] Tool Call: {tool_name} | Args: {tool_args}")

                # MCP ë„êµ¬ ì‹¤í–‰
                result = await session.call_tool(tool_name, arguments=tool_args)
                tool_output = result.content[0].text

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": tool_output
                })

            # 5. ìµœì¢… ë‹µë³€ ìƒì„±
            final_res = client.chat.completions.create(
                model="gemini-2.0-flash-exp",
                messages=messages
            )
            return final_res.choices[0].message.content

        return assistant_msg.content

    except Exception as e:
        print(f"âŒ Agent Error: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."